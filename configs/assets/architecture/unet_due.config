from functools import partial

import numpy as np
import torch

from dpipe.train import Switch
from dpipe.train.policy import Schedule
from dpipe.predict import add_extract_dims, divisible_shape
from dpipe.torch import inference_step
from dpipe.predict.shape import patches_grid
from dpipe.torch.functional import weighted_cross_entropy_with_logits
from gpytorch.mlls import VariationalELBO
from gpytorch.likelihoods import SoftmaxLikelihood
from ood.torch.module.unet_spectral_norm_mri import UNetSpectralNormFeatureExtractor
from ood.torch.module.dkl import initial_values, GP, DKL
from ood.torch.module.unet_due import UNetDUE
from ood.batch_iter import SPATIAL_DIMS
from ood.loss import combined_loss_weighted, cosine_lr_schedule_fn


# model

# DUE parameters
num_classes = 1 
n_inducing_points = 2 
kernel = 'RBF'
num_train_voxels = len(train_ids) * 256 * 206 * 269 # TODO remove hard-coded numbers

task_ndim = 3
n_filters = 16
x_patch_size = y_patch_size = np.array([80, ] * 3)
feature_extractor = UNetSpectralNormFeatureExtractor(ndim=task_ndim, n_chans_in=n_chans_in, 
                                                n_chans_out=n_chans_out, n_filters_init=n_filters,
                                                return_features_from=(19, ), 
                                                x_patch_size=x_patch_size[0])
   
initial_inducing_points, initial_lengthscale = initial_values(
    batch_iter=batch_iter, 
    feature_extractor=feature_extractor, 
    device=device, 
    n_inducing_points=n_inducing_points
)

gp = GP(
    num_outputs=num_classes,
    initial_lengthscale=initial_lengthscale,
    initial_inducing_points=initial_inducing_points,
    kernel=kernel,
)

architecture = DKL(feature_extractor, gp)
likelihood = SoftmaxLikelihood(num_classes=num_classes, mixing_weights=False).to(device)

# loss
elbo_fn = VariationalELBO(likelihood, gp, num_data=num_train_voxels)
criterion = lambda x, y: -elbo_fn(x, y)
        

batch_size = 11

# optimizer
batches_per_epoch = 40
n_epochs = 250
lr_min = 1e-6
lr_opt = 0.005
last_linear_epoch = max(n_epochs // 10, 1)
lr = Switch(initial=lr_min,
            epoch_to_value={i: cosine_lr_schedule_fn(i, lr_max=lr_opt, lr_min=lr_min,
                                                     last_linear_epoch=last_linear_epoch, n_epochs=n_epochs)
                            for i in range(n_epochs)})

weight_decay = 1e-4
optimizer = torch.optim.SGD(architecture.parameters(), lr=lr_min, momentum=0.9, nesterov=True,
                            weight_decay=weight_decay)


# predict
pred_patch_size = x_patch_size
pred_patch_stride = pred_patch_size // 2


@add_extract_dims(2)  # 3D -> (5D -> predict -> 5D) -> 3D
@patches_grid(pred_patch_size, pred_patch_stride, axis=SPATIAL_DIMS)  # image -> iter{patches} -> average
@divisible_shape(divisor=[16, ] * 3, padding_values=np.min, axis=SPATIAL_DIMS)
def predict(image):
    return inference_step(image, architecture=architecture, activation=torch.sigmoid, amp=amp)


@add_extract_dims(2)  # 3D -> (5D -> predict -> 5D) -> 3D
@patches_grid(pred_patch_size, pred_patch_stride, axis=SPATIAL_DIMS)  # image -> iter{patches} -> average
@divisible_shape(divisor=[16, ] * 3, padding_values=np.min, axis=SPATIAL_DIMS)
def predict_logit(x):
    return inference_step(x, architecture=architecture, amp=amp)