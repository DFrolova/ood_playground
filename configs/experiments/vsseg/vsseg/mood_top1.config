from ....assets.core_lidc import *
from ....assets.utils.no_pred_logits import *
from ....assets.dataset.vsseg_setup import *
from ....assets.dataset.vsseg import *
from ....assets.batch_iter.patch_sampling_3d_axial_cutpaste import *
from ....assets.architecture.unet_lidc import *
from ....assets.cross_val.cv import *
from ....assets.metric.metrics_3d import *

from dpipe.train import Switch
from ood.loss import cosine_lr_schedule_fn


max_cache_size = 170
seed = 0

mood_top1_top100mean = lambda true, pred: np.mean(pred.ravel()[np.argsort(pred, axis=None)[-100:]])
mood_top1_mean = lambda true, pred: np.mean(pred)
mood_top1_max = lambda true, pred: np.max(pred)


final_metrics_lidc = {'mood_top1_top100mean': mood_top1_top100mean,
                      'mood_top1_mean': mood_top1_mean,
                      'mood_top1_max': mood_top1_max, }

final_metrics_cc359_midrc = {'mood_top1_top100mean': mood_top1_top100mean,
                             'mood_top1_mean': mood_top1_mean,
                             'mood_top1_max': mood_top1_max, }

max_anomaly_size = 64
batch_size = 4

device = 'cuda'

lr_base = 1e-3
lr_min = 1e-7
last_linear_epoch = n_epochs // 10 + 1
lr = Switch(initial=lr_min,
            epoch_to_value={i: cosine_lr_schedule_fn(i, lr_max=lr_base, lr_min=lr_min,
                                                     last_linear_epoch=last_linear_epoch, n_epochs=n_epochs)
                            for i in range(n_epochs)})

optimizer = torch.optim.Adam(architecture.parameters(), lr=lr_min, weight_decay=1e-4)
